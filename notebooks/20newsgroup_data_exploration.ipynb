{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "en_stop.append(\"com\")\n",
    "en_stop.append(\"can\")\n",
    "en_stop.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858\n",
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups(subset=\"train\", shuffle = True)\n",
    "# compile sample documents into a list\n",
    "text_corpus = newsgroups.data[1]\n",
    "print(len(text_corpus))\n",
    "print(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a -door sports car, looked to be from the late s/\\nearly s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corpus = newsgroups.data[0]\n",
    "text_corpus = re.sub('From.*\\\\n', '', text_corpus)\n",
    "text_corpus = re.sub('Subject.*\\\\n', '', text_corpus)\n",
    "text_corpus = re.sub('[nN][nN][tT][pP]-Posting-Host.*\\\\n', '', text_corpus)\n",
    "text_corpus = re.sub('Organization.*\\\\n', '', text_corpus)\n",
    "text_corpus = re.sub('Lines.*\\\\n', '', text_corpus)\n",
    "text_corpus = re.sub('Summary.*\\\\n', '', text_corpus)\n",
    "text_corpus = re.sub('Keywords.*\\\\n', '', text_corpus)\n",
    "text_corpus = re.sub('Article-I.D.*\\\\n', '', text_corpus)\n",
    "text_corpus = re.sub('<.*>', '', text_corpus)\n",
    "text_corpus = re.sub('\\d*', '', text_corpus)\n",
    "text_corpus = re.sub('--*\\n', '', text_corpus)\n",
    "text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'r', 'o', 'm', ':', ' ', 'l', 'e', 'r', 'x', 's', 't', '', 'w', 'a', 'm', '.', 'u', 'm', 'd', '.', 'e', 'd', 'u', ' ', '(', 'w', 'h', 'e', 'r', 'e', \"'\", 's', ' ', 'm', 'y', ' ', 't', 'h', 'i', 'n', 'g', ')', '\\n', 'S', 'u', 'b', 'j', 'e', 'c', 't', ':', ' ', 'W', 'H', 'A', 'T', ' ', 'c', 'a', 'r', ' ', 'i', 's', ' ', 't', 'h', 'i', 's', '!', '?', '\\n', 'N', 'n', 't', 'p', '-', 'P', 'o', 's', 't', 'i', 'n', 'g', '-', 'H', 'o', 's', 't', ':', ' ', 'r', 'a', 'c', '3', '.', 'w', 'a', 'm', '.', 'u', 'm', 'd', '.', 'e', 'd', 'u', '\\n', 'O', 'r', 'g', 'a', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ':', ' ', 'U', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', ' ', 'o', 'f', ' ', 'M', 'a', 'r', 'y', 'l', 'a', 'n', 'd', ',', ' ', 'C', 'o', 'l', 'l', 'e', 'g', 'e', ' ', 'P', 'a', 'r', 'k', '\\n', 'L', 'i', 'n', 'e', 's', ':', ' ', '1', '5', '\\n', '\\n', ' ', 'I', ' ', 'w', 'a', 's', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'i', 'n', 'g', ' ', 'i', 'f', ' ', 'a', 'n', 'y', 'o', 'n', 'e', ' ', 'o', 'u', 't', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 'e', 'n', 'l', 'i', 'g', 'h', 't', 'e', 'n', ' ', 'm', 'e', ' ', 'o', 'n', ' ', 't', 'h', 'i', 's', ' ', 'c', 'a', 'r', ' ', 'I', ' ', 's', 'a', 'w', '\\n', 't', 'h', 'e', ' ', 'o', 't', 'h', 'e', 'r', ' ', 'd', 'a', 'y', '.', ' ', 'I', 't', ' ', 'w', 'a', 's', ' ', 'a', ' ', '2', '-', 'd', 'o', 'o', 'r', ' ', 's', 'p', 'o', 'r', 't', 's', ' ', 'c', 'a', 'r', ',', ' ', 'l', 'o', 'o', 'k', 'e', 'd', ' ', 't', 'o', ' ', 'b', 'e', ' ', 'f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 'l', 'a', 't', 'e', ' ', '6', '0', 's', '/', '\\n', 'e', 'a', 'r', 'l', 'y', ' ', '7', '0', 's', '.', ' ', 'I', 't', ' ', 'w', 'a', 's', ' ', 'c', 'a', 'l', 'l', 'e', 'd', ' ', 'a', ' ', 'B', 'r', 'i', 'c', 'k', 'l', 'i', 'n', '.', ' ', 'T', 'h', 'e', ' ', 'd', 'o', 'o', 'r', 's', ' ', 'w', 'e', 'r', 'e', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 's', 'm', 'a', 'l', 'l', '.', ' ', 'I', 'n', ' ', 'a', 'd', 'd', 'i', 't', 'i', 'o', 'n', ',', '\\n', 't', 'h', 'e', ' ', 'f', 'r', 'o', 'n', 't', ' ', 'b', 'u', 'm', 'p', 'e', 'r', ' ', 'w', 'a', 's', ' ', 's', 'e', 'p', 'a', 'r', 'a', 't', 'e', ' ', 'f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 'r', 'e', 's', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'b', 'o', 'd', 'y', '.', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', '\\n', 'a', 'l', 'l', ' ', 'I', ' ', 'k', 'n', 'o', 'w', '.', ' ', 'I', 'f', ' ', 'a', 'n', 'y', 'o', 'n', 'e', ' ', 'c', 'a', 'n', ' ', 't', 'e', 'l', 'l', 'm', 'e', ' ', 'a', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'n', 'a', 'm', 'e', ',', ' ', 'e', 'n', 'g', 'i', 'n', 'e', ' ', 's', 'p', 'e', 'c', 's', ',', ' ', 'y', 'e', 'a', 'r', 's', '\\n', 'o', 'f', ' ', 'p', 'r', 'o', 'd', 'u', 'c', 't', 'i', 'o', 'n', ',', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 't', 'h', 'i', 's', ' ', 'c', 'a', 'r', ' ', 'i', 's', ' ', 'm', 'a', 'd', 'e', ',', ' ', 'h', 'i', 's', 't', 'o', 'r', 'y', ',', ' ', 'o', 'r', ' ', 'w', 'h', 'a', 't', 'e', 'v', 'e', 'r', ' ', 'i', 'n', 'f', 'o', ' ', 'y', 'o', 'u', '\\n', 'h', 'a', 'v', 'e', ' ', 'o', 'n', ' ', 't', 'h', 'i', 's', ' ', 'f', 'u', 'n', 'k', 'y', ' ', 'l', 'o', 'o', 'k', 'i', 'n', 'g', ' ', 'c', 'a', 'r', ',', ' ', 'p', 'l', 'e', 'a', 's', 'e', ' ', 'e', '-', 'm', 'a', 'i', 'l', '.', '\\n', '\\n', 'T', 'h', 'a', 'n', 'k', 's', ',', '\\n', '-', ' ', 'I', 'L', '\\n', ' ', ' ', ' ', '-', '-', '-', '-', ' ', 'b', 'r', 'o', 'u', 'g', 'h', 't', ' ', 't', 'o', ' ', 'y', 'o', 'u', ' ', 'b', 'y', ' ', 'y', 'o', 'u', 'r', ' ', 'n', 'e', 'i', 'g', 'h', 'b', 'o', 'r', 'h', 'o', 'o', 'd', ' ', 'L', 'e', 'r', 'x', 's', 't', ' ', '-', '-', '-', '-', '\\n', '\\n', '\\n', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "text_corpus = [re.sub('\\S*@\\S*\\s?', '', doc) for doc in text_corpus] #removing email addresses\n",
    "#text_corpus = [re.sub('\\s+', ' ', doc) for doc in text_corpus] #removing newline characters\n",
    "# text_corpus = [re.sub(\"\\'\", \"\", doc) for doc in text_corpus] #removing single quote characters\n",
    "print(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "tagged = []\n",
    "\n",
    "# loop through document list\n",
    "for count, doc in enumerate(text_corpus):\n",
    "    #print(count/len(doc_set))\n",
    "    # clean and tokenize document string\n",
    "    \n",
    "    raw = doc.lower()\n",
    "    #tokens = tokenizer.tokenize(raw)\n",
    "    tokens = gensim.utils.simple_preprocess(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if (not i in en_stop) and (len(i) >= 3)]\n",
    "\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "    print(stemmed_tokens)\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=8, id2word=dictionary, passes=5)\n",
    "\n",
    "ldamodel.print_topics(num_topics = 8, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
